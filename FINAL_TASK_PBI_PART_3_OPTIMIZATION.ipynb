{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oUQdBvVzpVW",
        "outputId": "49e1c54b-dc0e-4f47-86a2-bb0cc0670fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Starting Feature Engineering Optimization...\n",
            "Original dataset shape: (1449, 248)\n",
            "\n",
            "📊 STEP 1: Creating Advanced Features...\n",
            "Creating External Source features...\n",
            "✅ External Source features created\n",
            "Creating age-based features...\n",
            "✅ Age-based features created\n",
            "Creating income-based features...\n",
            "✅ Income-based features created\n",
            "Creating interaction features...\n",
            "✅ Interaction features created\n",
            "Creating composite risk scores...\n",
            "✅ Risk scores created\n",
            "After advanced feature engineering: (1449, 267)\n",
            "\n",
            "🔍 STEP 2: Multicollinearity Analysis...\n",
            "Analyzing 244 numerical features for multicollinearity...\n",
            "Analyzing VIF for 157 key features...\n",
            "Batch 1: Found 44 high VIF features\n",
            "Batch 2: Found 47 high VIF features\n",
            "Found 91 features with VIF > 10\n",
            "High VIF features to consider removing:\n",
            "  - BUREAU_CNT_CREDIT_PROLONG_count\n",
            "  - BUREAU_DAYS_CREDIT_ENDDATE_sum\n",
            "  - BUREAU_CREDIT_DAY_OVERDUE_min\n",
            "  - BUREAU_AMT_CREDIT_SUM_max\n",
            "  - BUREAU_AMT_CREDIT_MAX_OVERDUE_min\n",
            "  - BUREAU_AMT_CREDIT_MAX_OVERDUE_mean_x\n",
            "  - PREV_AMT_APPLICATION_max\n",
            "  - AMT_INCOME_TOTAL\n",
            "  - EXT_SOURCE_1\n",
            "  - BUREAU_AMT_CREDIT_SUM_std\n",
            "\n",
            "📈 STEP 3: Correlation-based Feature Selection...\n",
            "Found 66 highly correlated pairs (>0.95)\n",
            "Recommending to drop 41 highly correlated features\n",
            "\n",
            "🎯 STEP 4: Final Feature Selection...\n",
            "Final recommendation: Drop 121 features\n",
            "This will reduce dataset from 267 to 146 columns\n",
            "✅ Optimized dataset shape: (1449, 146)\n",
            "\n",
            "🏷️ STEP 5: Categorical Encoding Preparation...\n",
            "Found 18 categorical columns\n",
            "High cardinality (>10 unique): 2 columns\n",
            "Low cardinality (≤10 unique): 16 columns\n",
            "After encoding: (1449, 198)\n",
            "\n",
            "✅ STEP 6: Final Validation...\n",
            "Final dataset shape: (1449, 198)\n",
            "Missing values: 0\n",
            "Duplicates: 0\n",
            "Target distribution:\n",
            "TARGET\n",
            "0.0    0.929607\n",
            "1.0    0.070393\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "📊 Feature Engineering Summary:\n",
            "  Total Features: 198\n",
            "  Numerical Features: 128\n",
            "  Categorical Features: 2\n",
            "  New Advanced Features: 22\n",
            "  Features Dropped: 121\n",
            "\n",
            "💾 Optimized dataset saved as: app_train_optimized_final.csv\n",
            "\n",
            "🎉 Feature Engineering Optimization Complete!\n",
            "Dataset is now ready for modeling with:\n",
            "  ✅ Advanced engineered features\n",
            "  ✅ Reduced multicollinearity\n",
            "  ✅ Proper categorical encoding\n",
            "  ✅ Optimized feature count\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# FEATURE ENGINEERING OPTIMIZATION GUIDE\n",
        "# Tambahkan code ini SETELAH merge selesai\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🔧 Starting Feature Engineering Optimization...\")\n",
        "\n",
        "# Load dataset hasil merge Anda\n",
        "app_train_fe = pd.read_csv(\"app_train_with_features_cleaned1.csv\")\n",
        "print(f\"Original dataset shape: {app_train_fe.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 1: ADVANCED FEATURE ENGINEERING\n",
        "# ==========================================\n",
        "print(\"\\n📊 STEP 1: Creating Advanced Features...\")\n",
        "\n",
        "# 1.1 External Source Combinations (PRIORITAS TINGGI)\n",
        "print(\"Creating External Source features...\")\n",
        "ext_source_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
        "\n",
        "# Pastikan kolom ada\n",
        "for col in ext_source_cols:\n",
        "    if col not in app_train_fe.columns:\n",
        "        print(f\"Warning: {col} not found, creating dummy column\")\n",
        "        app_train_fe[col] = np.nan\n",
        "\n",
        "# Kombinasi External Sources\n",
        "app_train_fe['EXT_SOURCE_MEAN'] = app_train_fe[ext_source_cols].mean(axis=1)\n",
        "app_train_fe['EXT_SOURCE_STD'] = app_train_fe[ext_source_cols].std(axis=1)\n",
        "app_train_fe['EXT_SOURCE_MAX'] = app_train_fe[ext_source_cols].max(axis=1)\n",
        "app_train_fe['EXT_SOURCE_MIN'] = app_train_fe[ext_source_cols].min(axis=1)\n",
        "\n",
        "# Weighted combination (berdasarkan korelasi dengan target dari EDA Anda)\n",
        "app_train_fe['EXT_SOURCE_WEIGHTED'] = (app_train_fe['EXT_SOURCE_1'] * 0.4 +\n",
        "                                      app_train_fe['EXT_SOURCE_2'] * 0.35 +\n",
        "                                      app_train_fe['EXT_SOURCE_3'] * 0.25)\n",
        "\n",
        "# Missing indicators untuk External Sources\n",
        "for i in [1, 2, 3]:\n",
        "    app_train_fe[f'EXT_SOURCE_{i}_MISSING'] = app_train_fe[f'EXT_SOURCE_{i}'].isnull().astype(int)\n",
        "\n",
        "print(\"✅ External Source features created\")\n",
        "\n",
        "# 1.2 Age-based Features\n",
        "print(\"Creating age-based features...\")\n",
        "if 'DAYS_BIRTH' in app_train_fe.columns:\n",
        "    app_train_fe['YEARS_BIRTH'] = app_train_fe['DAYS_BIRTH'] / -365.25\n",
        "\n",
        "    # Age groups berdasarkan insight bisnis Indonesia\n",
        "    app_train_fe['AGE_GROUP'] = pd.cut(app_train_fe['YEARS_BIRTH'],\n",
        "                                      bins=[0, 25, 35, 45, 55, 100],\n",
        "                                      labels=['Young', 'Adult', 'Middle', 'Senior', 'Elder'])\n",
        "\n",
        "    # Employment stability\n",
        "    if 'DAYS_EMPLOYED' in app_train_fe.columns:\n",
        "        app_train_fe['YEARS_EMPLOYED'] = app_train_fe['DAYS_EMPLOYED'] / -365.25\n",
        "        app_train_fe['EMPLOYMENT_STABILITY'] = app_train_fe['YEARS_EMPLOYED'] / app_train_fe['YEARS_BIRTH']\n",
        "\n",
        "        # Handle infinite values\n",
        "        app_train_fe['EMPLOYMENT_STABILITY'] = app_train_fe['EMPLOYMENT_STABILITY'].replace([np.inf, -np.inf], np.nan)\n",
        "        app_train_fe['EMPLOYMENT_STABILITY'] = app_train_fe['EMPLOYMENT_STABILITY'].fillna(0)\n",
        "\n",
        "print(\"✅ Age-based features created\")\n",
        "\n",
        "# 1.3 Income-based Features\n",
        "print(\"Creating income-based features...\")\n",
        "if 'AMT_INCOME_TOTAL' in app_train_fe.columns and 'CNT_FAM_MEMBERS' in app_train_fe.columns:\n",
        "    app_train_fe['INCOME_PER_FAMILY_MEMBER'] = app_train_fe['AMT_INCOME_TOTAL'] / (app_train_fe['CNT_FAM_MEMBERS'] + 1)\n",
        "\n",
        "# Income categories berdasarkan distribusi dari EDA Anda\n",
        "if 'AMT_INCOME_TOTAL' in app_train_fe.columns:\n",
        "    app_train_fe['INCOME_CATEGORY'] = pd.cut(app_train_fe['AMT_INCOME_TOTAL'],\n",
        "                                            bins=[0, 100000, 200000, 300000, np.inf],\n",
        "                                            labels=['Low', 'Medium', 'High', 'Very_High'])\n",
        "\n",
        "print(\"✅ Income-based features created\")\n",
        "\n",
        "# 1.4 Interaction Features (High Impact)\n",
        "print(\"Creating interaction features...\")\n",
        "\n",
        "# Gender-Car interaction (berdasarkan EDA Anda)\n",
        "if 'CODE_GENDER' in app_train_fe.columns and 'FLAG_OWN_CAR' in app_train_fe.columns:\n",
        "    app_train_fe['GENDER_CAR_INTERACTION'] = app_train_fe['CODE_GENDER'].astype(str) + '_' + app_train_fe['FLAG_OWN_CAR'].astype(str)\n",
        "\n",
        "# Income-Credit interaction\n",
        "if 'AMT_INCOME_TOTAL' in app_train_fe.columns and 'AMT_CREDIT' in app_train_fe.columns:\n",
        "    app_train_fe['INCOME_CREDIT_INTERACTION'] = app_train_fe['AMT_INCOME_TOTAL'] * app_train_fe['AMT_CREDIT'] / 1e12  # Scale down\n",
        "\n",
        "# Age-Income interaction\n",
        "if 'YEARS_BIRTH' in app_train_fe.columns and 'AMT_INCOME_TOTAL' in app_train_fe.columns:\n",
        "    app_train_fe['AGE_INCOME_INTERACTION'] = app_train_fe['YEARS_BIRTH'] * app_train_fe['AMT_INCOME_TOTAL'] / 1e6  # Scale down\n",
        "\n",
        "print(\"✅ Interaction features created\")\n",
        "\n",
        "# 1.5 Risk Score Engineering\n",
        "print(\"Creating composite risk scores...\")\n",
        "\n",
        "def create_risk_score(row):\n",
        "    \"\"\"Composite risk score berdasarkan findings EDA Anda\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Gender risk (Male lebih berisiko dari EDA Anda)\n",
        "    if row.get('CODE_GENDER') == 'M':\n",
        "        score += 1\n",
        "\n",
        "    # Contract type risk (Cash loans lebih berisiko)\n",
        "    if row.get('NAME_CONTRACT_TYPE') == 'Cash loans':\n",
        "        score += 1\n",
        "\n",
        "    # Payment burden risk\n",
        "    if pd.notna(row.get('PAYMENT_TO_INCOME_RATIO')) and row.get('PAYMENT_TO_INCOME_RATIO', 0) > 0.4:\n",
        "        score += 2\n",
        "\n",
        "    # Age risk (older customers lebih berisiko dari EDA)\n",
        "    if pd.notna(row.get('YEARS_BIRTH')) and row.get('YEARS_BIRTH', 0) > 50:\n",
        "        score += 1\n",
        "\n",
        "    # External source missing risk\n",
        "    if pd.isna(row.get('EXT_SOURCE_1')) and pd.isna(row.get('EXT_SOURCE_2')):\n",
        "        score += 2\n",
        "\n",
        "    # Car ownership (slight risk difference dari EDA)\n",
        "    if row.get('FLAG_OWN_CAR') == 'N':\n",
        "        score += 0.5\n",
        "\n",
        "    return score\n",
        "\n",
        "app_train_fe['COMPOSITE_RISK_SCORE'] = app_train_fe.apply(create_risk_score, axis=1)\n",
        "\n",
        "# Regional risk score\n",
        "if 'REGION_RATING_CLIENT' in app_train_fe.columns and 'REGION_RATING_CLIENT_W_CITY' in app_train_fe.columns:\n",
        "    app_train_fe['REGION_RISK_SCORE'] = (app_train_fe['REGION_RATING_CLIENT'] +\n",
        "                                        app_train_fe['REGION_RATING_CLIENT_W_CITY']) / 2\n",
        "\n",
        "print(\"✅ Risk scores created\")\n",
        "\n",
        "print(f\"After advanced feature engineering: {app_train_fe.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: MULTICOLLINEARITY ANALYSIS & FEATURE SELECTION\n",
        "# ==========================================\n",
        "print(\"\\n🔍 STEP 2: Multicollinearity Analysis...\")\n",
        "\n",
        "# Identify numerical columns (exclude IDs and target)\n",
        "numerical_cols = app_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
        "exclude_cols = ['SK_ID_CURR', 'TARGET'] + [col for col in numerical_cols if 'ID' in col.upper()]\n",
        "numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
        "\n",
        "print(f\"Analyzing {len(numerical_cols)} numerical features for multicollinearity...\")\n",
        "\n",
        "# Calculate VIF untuk subset features (VIF computation expensive untuk 200+ features)\n",
        "def calculate_vif_batch(df, features, batch_size=50):\n",
        "    \"\"\"Calculate VIF in batches to handle large feature sets\"\"\"\n",
        "    high_vif_features = []\n",
        "\n",
        "    for i in range(0, len(features), batch_size):\n",
        "        batch_features = features[i:i+batch_size]\n",
        "\n",
        "        # Ensure no missing values untuk VIF calculation\n",
        "        batch_df = df[batch_features].fillna(df[batch_features].median())\n",
        "\n",
        "        # Remove constant columns\n",
        "        variable_cols = []\n",
        "        for col in batch_features:\n",
        "            if batch_df[col].nunique() > 1:\n",
        "                variable_cols.append(col)\n",
        "\n",
        "        if len(variable_cols) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Calculate VIF\n",
        "            vif_data = pd.DataFrame()\n",
        "            vif_data[\"Feature\"] = variable_cols\n",
        "            vif_data[\"VIF\"] = [variance_inflation_factor(batch_df[variable_cols].values, j)\n",
        "                              for j in range(len(variable_cols))]\n",
        "\n",
        "            # Flag high VIF features\n",
        "            high_vif_batch = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
        "            high_vif_features.extend(high_vif_batch)\n",
        "\n",
        "            print(f\"Batch {i//batch_size + 1}: Found {len(high_vif_batch)} high VIF features\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(set(high_vif_features))\n",
        "\n",
        "# Calculate VIF untuk key feature groups\n",
        "key_features = [col for col in numerical_cols if any(keyword in col.upper() for keyword in\n",
        "               ['AMT_', 'DAYS_', 'EXT_SOURCE', 'BUREAU_', 'PREV_', 'PAYMENT_', 'RATIO'])]\n",
        "\n",
        "print(f\"Analyzing VIF for {len(key_features)} key features...\")\n",
        "high_vif_features = calculate_vif_batch(app_train_fe, key_features[:100])  # Limit untuk performance\n",
        "\n",
        "print(f\"Found {len(high_vif_features)} features with VIF > 10\")\n",
        "if high_vif_features:\n",
        "    print(\"High VIF features to consider removing:\")\n",
        "    for feature in high_vif_features[:10]:  # Show first 10\n",
        "        print(f\"  - {feature}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: CORRELATION-BASED FEATURE REMOVAL\n",
        "# ==========================================\n",
        "print(\"\\n📈 STEP 3: Correlation-based Feature Selection...\")\n",
        "\n",
        "# Calculate correlation matrix untuk numerical features\n",
        "correlation_matrix = app_train_fe[numerical_cols[:100]].corr()  # Limit untuk performance\n",
        "\n",
        "# Find highly correlated pairs\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = abs(correlation_matrix.iloc[i, j])\n",
        "        if corr_value > 0.95:  # Very high correlation threshold\n",
        "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value))\n",
        "\n",
        "print(f\"Found {len(high_corr_pairs)} highly correlated pairs (>0.95)\")\n",
        "\n",
        "# Recommend features to drop (keep the one with better business meaning)\n",
        "features_to_drop = []\n",
        "for pair in high_corr_pairs:\n",
        "    feature1, feature2, corr = pair\n",
        "\n",
        "    # Business logic untuk memilih feature mana yang di-drop\n",
        "    if 'GOODS_PRICE' in feature1 and 'CREDIT' in feature2:\n",
        "        features_to_drop.append(feature1)  # Drop GOODS_PRICE, keep CREDIT\n",
        "    elif 'GOODS_PRICE' in feature2 and 'CREDIT' in feature1:\n",
        "        features_to_drop.append(feature2)\n",
        "    elif 'sum' in feature1 and 'mean' in feature2:\n",
        "        features_to_drop.append(feature1)  # Keep mean over sum untuk interpretability\n",
        "    elif 'sum' in feature2 and 'mean' in feature1:\n",
        "        features_to_drop.append(feature2)\n",
        "    else:\n",
        "        features_to_drop.append(pair[1])  # Default: drop second feature\n",
        "\n",
        "features_to_drop = list(set(features_to_drop))\n",
        "print(f\"Recommending to drop {len(features_to_drop)} highly correlated features\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: FINAL FEATURE SELECTION\n",
        "# ==========================================\n",
        "print(\"\\n🎯 STEP 4: Final Feature Selection...\")\n",
        "\n",
        "# Combine all features to drop\n",
        "all_features_to_drop = list(set(high_vif_features + features_to_drop))\n",
        "\n",
        "# Keep essential features regardless of VIF/correlation\n",
        "essential_features = [\n",
        "    'TARGET', 'SK_ID_CURR',\n",
        "    'PAYMENT_TO_INCOME_RATIO', 'LOAN_TO_VALUE_RATIO', 'INCOME_ADEQUACY_RATIO',\n",
        "    'EXT_SOURCE_WEIGHTED', 'EXT_SOURCE_MEAN',\n",
        "    'COMPOSITE_RISK_SCORE', 'YEARS_BIRTH', 'YEARS_EMPLOYED',\n",
        "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
        "    'CODE_GENDER', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY'\n",
        "]\n",
        "\n",
        "# Remove essential features dari drop list\n",
        "final_features_to_drop = [f for f in all_features_to_drop if f not in essential_features]\n",
        "\n",
        "print(f\"Final recommendation: Drop {len(final_features_to_drop)} features\")\n",
        "print(f\"This will reduce dataset from {app_train_fe.shape[1]} to {app_train_fe.shape[1] - len(final_features_to_drop)} columns\")\n",
        "\n",
        "# Apply feature selection\n",
        "app_train_optimized = app_train_fe.drop(columns=final_features_to_drop, errors='ignore')\n",
        "\n",
        "print(f\"✅ Optimized dataset shape: {app_train_optimized.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: CATEGORICAL ENCODING PREPARATION\n",
        "# ==========================================\n",
        "print(\"\\n🏷️ STEP 5: Categorical Encoding Preparation...\")\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = app_train_optimized.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Found {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "# Separate by cardinality\n",
        "high_cardinality_cats = []  # For target encoding\n",
        "low_cardinality_cats = []   # For one-hot encoding\n",
        "\n",
        "for col in categorical_cols:\n",
        "    unique_count = app_train_optimized[col].nunique()\n",
        "    if unique_count > 10:\n",
        "        high_cardinality_cats.append(col)\n",
        "    else:\n",
        "        low_cardinality_cats.append(col)\n",
        "\n",
        "print(f\"High cardinality (>10 unique): {len(high_cardinality_cats)} columns\")\n",
        "print(f\"Low cardinality (≤10 unique): {len(low_cardinality_cats)} columns\")\n",
        "\n",
        "# Apply One-Hot Encoding untuk low cardinality\n",
        "if low_cardinality_cats:\n",
        "    app_train_encoded = pd.get_dummies(app_train_optimized,\n",
        "                                      columns=low_cardinality_cats,\n",
        "                                      prefix=low_cardinality_cats,\n",
        "                                      drop_first=True)\n",
        "else:\n",
        "    app_train_encoded = app_train_optimized.copy()\n",
        "\n",
        "# Untuk high cardinality, gunakan Label Encoding (Target encoding bisa dilakukan nanti)\n",
        "label_encoders = {}\n",
        "for col in high_cardinality_cats:\n",
        "    le = LabelEncoder()\n",
        "    app_train_encoded[col + '_ENCODED'] = le.fit_transform(app_train_encoded[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    # Keep original column untuk reference\n",
        "\n",
        "print(f\"After encoding: {app_train_encoded.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: FINAL VALIDATION & SAVE\n",
        "# ==========================================\n",
        "print(\"\\n✅ STEP 6: Final Validation...\")\n",
        "\n",
        "# Final checks\n",
        "print(f\"Final dataset shape: {app_train_encoded.shape}\")\n",
        "print(f\"Missing values: {app_train_encoded.isnull().sum().sum()}\")\n",
        "print(f\"Duplicates: {app_train_encoded.duplicated().sum()}\")\n",
        "\n",
        "# Target distribution\n",
        "if 'TARGET' in app_train_encoded.columns:\n",
        "    print(f\"Target distribution:\\n{app_train_encoded['TARGET'].value_counts(normalize=True)}\")\n",
        "\n",
        "# Feature categories summary\n",
        "feature_summary = {\n",
        "    'Total Features': app_train_encoded.shape[1],\n",
        "    'Numerical Features': len(app_train_encoded.select_dtypes(include=[np.number]).columns),\n",
        "    'Categorical Features': len(app_train_encoded.select_dtypes(include=['object']).columns),\n",
        "    'New Advanced Features': len([col for col in app_train_encoded.columns if\n",
        "                                 any(keyword in col for keyword in ['EXT_SOURCE_', 'RISK_SCORE', 'INTERACTION', 'AGE_GROUP'])]),\n",
        "    'Features Dropped': len(final_features_to_drop)\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Feature Engineering Summary:\")\n",
        "for key, value in feature_summary.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save optimized dataset\n",
        "optimized_path = \"app_train_optimized_final.csv\"\n",
        "app_train_encoded.to_csv(optimized_path, index=False)\n",
        "print(f\"\\n💾 Optimized dataset saved as: {optimized_path}\")\n",
        "\n",
        "print(\"\\n🎉 Feature Engineering Optimization Complete!\")\n",
        "print(\"Dataset is now ready for modeling with:\")\n",
        "print(\"  ✅ Advanced engineered features\")\n",
        "print(\"  ✅ Reduced multicollinearity\")\n",
        "print(\"  ✅ Proper categorical encoding\")\n",
        "print(\"  ✅ Optimized feature count\")"
      ]
    }
  ]
}