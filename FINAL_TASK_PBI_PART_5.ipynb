{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj2V5rPGPNGx",
        "outputId": "f256dfef-8c38-4cfd-8cac-2b8825fa15fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Loading saved model package...\n",
            "âœ… Loaded Hyperparameter Tuned model with AUC: 0.7374\n",
            "ðŸ“‚ Loading application_test.csv...\n",
            "ðŸ”§ Preprocessing new data...\n",
            "âš ï¸  Adding missing features: 85\n",
            "âš ï¸  Removing extra features: 76\n",
            "ðŸ”¤ Encoding categorical features...\n",
            "   Processing: OCCUPATION_TYPE\n",
            "      âœ… Successfully encoded\n",
            "   Processing: ORGANIZATION_TYPE\n",
            "      âš ï¸  Found 7 unknown categories\n",
            "      Examples: ['Industry: type 8', 'Industry: type 6', 'Trade: type 4']\n",
            "      âœ… Successfully encoded\n",
            "   Processing: AGE_GROUP\n",
            "      âš ï¸  Found 1 unknown categories\n",
            "      Examples: ['0']\n",
            "      âš ï¸  'Unknown' not in training classes for AGE_GROUP\n",
            "      Training classes: ['Adult', 'Middle', 'Senior', 'Elder', 'Young']\n",
            "      â†’ Mapping unknown values to: 'Adult'\n",
            "      âœ… Successfully encoded\n",
            "   Processing: INCOME_CATEGORY\n",
            "      âš ï¸  Found 1 unknown categories\n",
            "      Examples: ['0']\n",
            "      âš ï¸  'Unknown' not in training classes for INCOME_CATEGORY\n",
            "      Training classes: ['High', 'Medium', 'Very_High', 'Low']\n",
            "      â†’ Mapping unknown values to: 'High'\n",
            "      âœ… Successfully encoded\n",
            "âœ… Categorical encoding completed\n",
            "ðŸ”§ Imputing missing values...\n",
            "âš–ï¸ Scaling features...\n",
            "âœ… Preprocessing completed. Data shape: (48744, 130)\n",
            "\n",
            "ðŸ” Preprocessing Verification:\n",
            "Final data shape: (48744, 130)\n",
            "Any NaN values: 0\n",
            "Data type: float64\n",
            "\n",
            "ðŸ”® Making predictions...\n",
            "ðŸ“Š Creating submission file...\n",
            "âœ… Predictions saved! Shape: (48744, 3)\n",
            "\n",
            "ðŸ“ˆ Prediction Summary:\n",
            "Total predictions: 48744\n",
            "Predicted defaults (1): 241 (0.49%)\n",
            "Predicted no defaults (0): 48503 (99.51%)\n",
            "Average probability: 0.3283\n",
            "Probability range: 0.0000 - 1.0000\n",
            "\n",
            "ðŸŽ‰ All done! Check 'predictions_test.csv' for results.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the saved model package\n",
        "print(\"ðŸ“¦ Loading saved model package...\")\n",
        "model_package = joblib.load('home_credit_improved_model.pkl')\n",
        "\n",
        "model = model_package['model']\n",
        "scaler = model_package['scaler']\n",
        "label_encoders = model_package['label_encoders']\n",
        "feature_names = model_package['feature_names']\n",
        "train_median = model_package['train_median']\n",
        "threshold = model_package['threshold']\n",
        "\n",
        "print(f\"âœ… Loaded {model_package['method_used']} model with AUC: {model_package['performance_metrics']['auc']:.4f}\")\n",
        "\n",
        "# Load new data\n",
        "print(\"ðŸ“‚ Loading application_test.csv...\")\n",
        "new_data = pd.read_csv('application_test.csv')\n",
        "original_ids = new_data['SK_ID_CURR'].copy()\n",
        "\n",
        "# Pastikan hanya menggunakan feature yang sama dengan training\n",
        "print(\"ðŸ”§ Preprocessing new data...\")\n",
        "missing_features = set(feature_names) - set(new_data.columns)\n",
        "extra_features = set(new_data.columns) - set(feature_names)\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"âš ï¸  Adding missing features: {len(missing_features)}\")\n",
        "    for feature in missing_features:\n",
        "        new_data[feature] = 0\n",
        "\n",
        "if extra_features:\n",
        "    print(f\"âš ï¸  Removing extra features: {len(extra_features)}\")\n",
        "    new_data = new_data[feature_names]\n",
        "else:\n",
        "    new_data = new_data[feature_names]\n",
        "\n",
        "# SOLUSI PERBAIKAN: Robust Categorical Encoding\n",
        "print(\"ðŸ”¤ Encoding categorical features...\")\n",
        "\n",
        "def robust_label_encode(series, label_encoder, column_name):\n",
        "    \"\"\"\n",
        "    Robust label encoding yang handle unknown categories dengan baik\n",
        "    \"\"\"\n",
        "    print(f\"   Processing: {column_name}\")\n",
        "\n",
        "    # Step 1: Handle missing values\n",
        "    series_clean = series.fillna('Unknown')\n",
        "\n",
        "    # Step 2: Convert to string dan strip whitespace\n",
        "    series_clean = series_clean.astype(str).str.strip()\n",
        "\n",
        "    # Step 3: Get training categories\n",
        "    train_categories = set([str(x).strip() for x in label_encoder.classes_])\n",
        "    test_categories = set(series_clean.unique())\n",
        "\n",
        "    # Step 4: Identify unknown categories\n",
        "    unknown_categories = test_categories - train_categories\n",
        "\n",
        "    if unknown_categories:\n",
        "        print(f\"      âš ï¸  Found {len(unknown_categories)} unknown categories\")\n",
        "        print(f\"      Examples: {list(unknown_categories)[:3]}\")\n",
        "\n",
        "    # Step 5: PERBAIKAN - Cek apakah 'Unknown' ada di training classes\n",
        "    if 'Unknown' not in train_categories:\n",
        "        print(f\"      âš ï¸  'Unknown' not in training classes for {column_name}\")\n",
        "        print(f\"      Training classes: {list(train_categories)}\")\n",
        "\n",
        "        # Pilihan 1: Map ke kategori yang paling umum di training\n",
        "        most_common_class = label_encoder.classes_[0]  # Ambil class pertama sebagai default\n",
        "        print(f\"      â†’ Mapping unknown values to: '{most_common_class}'\")\n",
        "\n",
        "        # Map semua unknown categories ke most common class\n",
        "        series_encoded = series_clean.apply(\n",
        "            lambda x: most_common_class if x in unknown_categories else x\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # Jika 'Unknown' ada di training, map ke 'Unknown'\n",
        "        series_encoded = series_clean.apply(\n",
        "            lambda x: 'Unknown' if x in unknown_categories else x\n",
        "        )\n",
        "\n",
        "    # Step 6: Transform dengan encoder\n",
        "    try:\n",
        "        result = label_encoder.transform(series_encoded)\n",
        "        print(f\"      âœ… Successfully encoded\")\n",
        "        return result\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"      âŒ Still failed: {e}\")\n",
        "\n",
        "        # FALLBACK TERAKHIR: Manual encoding berdasarkan training classes\n",
        "        print(f\"      ðŸ”§ Using fallback: map to most frequent training class\")\n",
        "\n",
        "        # Buat mapping manual ke class pertama (biasanya most frequent)\n",
        "        fallback_class = label_encoder.classes_[0]\n",
        "        fallback_encoded = label_encoder.transform([fallback_class])[0]\n",
        "\n",
        "        # Map semua ke fallback class\n",
        "        result = np.full(len(series_encoded), fallback_encoded, dtype=int)\n",
        "        print(f\"      â†’ All values mapped to '{fallback_class}' (encoded: {fallback_encoded})\")\n",
        "        return result\n",
        "\n",
        "# Apply robust encoding untuk semua categorical features\n",
        "for col, le in label_encoders.items():\n",
        "    if col in new_data.columns:\n",
        "        new_data[col] = robust_label_encode(new_data[col], le, col)\n",
        "\n",
        "print(\"âœ… Categorical encoding completed\")\n",
        "\n",
        "# Handle missing values dengan median dari TRAINING set\n",
        "print(\"ðŸ”§ Imputing missing values...\")\n",
        "new_data = new_data.fillna(train_median)\n",
        "\n",
        "# Pastikan urutan kolom sama dengan training\n",
        "new_data = new_data[feature_names]\n",
        "\n",
        "# Scale features dengan scaler dari training\n",
        "print(\"âš–ï¸ Scaling features...\")\n",
        "new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "print(f\"âœ… Preprocessing completed. Data shape: {new_data_scaled.shape}\")\n",
        "\n",
        "# Verifikasi hasil preprocessing\n",
        "print(\"\\nðŸ” Preprocessing Verification:\")\n",
        "print(f\"Final data shape: {new_data_scaled.shape}\")\n",
        "print(f\"Any NaN values: {np.isnan(new_data_scaled).sum()}\")\n",
        "print(f\"Data type: {new_data_scaled.dtype}\")\n",
        "\n",
        "# Lanjutkan dengan prediksi\n",
        "print(\"\\nðŸ”® Making predictions...\")\n",
        "predictions_proba = model.predict_proba(new_data_scaled)[:, 1]\n",
        "predictions_binary = (predictions_proba >= threshold).astype(int)\n",
        "\n",
        "# Buat hasil prediksi\n",
        "print(\"ðŸ“Š Creating submission file...\")\n",
        "results_df = pd.DataFrame({\n",
        "    'SK_ID_CURR': original_ids,\n",
        "    'TARGET': predictions_binary,\n",
        "    'PREDICTION_PROBABILITY': predictions_proba\n",
        "})\n",
        "\n",
        "# Simpan hasil\n",
        "results_df.to_csv('predictions_test.csv', index=False)\n",
        "print(f\"âœ… Predictions saved! Shape: {results_df.shape}\")\n",
        "\n",
        "# Tampilkan ringkasan prediksi\n",
        "print(f\"\\nðŸ“ˆ Prediction Summary:\")\n",
        "print(f\"Total predictions: {len(results_df)}\")\n",
        "print(f\"Predicted defaults (1): {(predictions_binary == 1).sum()} ({(predictions_binary == 1).mean()*100:.2f}%)\")\n",
        "print(f\"Predicted no defaults (0): {(predictions_binary == 0).sum()} ({(predictions_binary == 0).mean()*100:.2f}%)\")\n",
        "print(f\"Average probability: {predictions_proba.mean():.4f}\")\n",
        "print(f\"Probability range: {predictions_proba.min():.4f} - {predictions_proba.max():.4f}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All done! Check 'predictions_test.csv' for results.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "print(\"ðŸŽ¯ Making predictions...\")\n",
        "predictions_proba = model.predict_proba(new_data_scaled)[:, 1]\n",
        "predictions_binary = (predictions_proba >= threshold).astype(int)\n",
        "\n",
        "# Create submission dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'SK_ID_CURR': original_ids,\n",
        "    'TARGET_PROBA': predictions_proba,\n",
        "    'TARGET_PREDICTION': predictions_binary\n",
        "})\n",
        "\n",
        "# Save results\n",
        "output_file = 'final_predictions.csv'\n",
        "results_df.to_csv(output_file, index=False)\n",
        "print(f\"âœ… Predictions saved to {output_file}\")\n",
        "print(f\"ðŸ“Š Prediction distribution:\")\n",
        "print(f\"   - Default predictions (1): {sum(predictions_binary)}\")\n",
        "print(f\"   - Non-default predictions (0): {len(predictions_binary) - sum(predictions_binary)}\")\n",
        "print(f\"   - Default rate: {sum(predictions_binary)/len(predictions_binary)*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkPp54VZZ5kd",
        "outputId": "58cda535-e321-47d3-e471-bdb0120fe221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Making predictions...\n",
            "âœ… Predictions saved to final_predictions.csv\n",
            "ðŸ“Š Prediction distribution:\n",
            "   - Default predictions (1): 241\n",
            "   - Non-default predictions (0): 48503\n",
            "   - Default rate: 0.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisis tambahan\n",
        "print(\"\\nðŸ“ˆ Prediction Analysis:\")\n",
        "print(f\"   - Min probability: {predictions_proba.min():.4f}\")\n",
        "print(f\"   - Max probability: {predictions_proba.max():.4f}\")\n",
        "print(f\"   - Mean probability: {predictions_proba.mean():.4f}\")\n",
        "\n",
        "# Simpan hasil dengan berbagai threshold untuk analisis bisnis\n",
        "for thresh in [0.3, 0.5, 0.7]:\n",
        "    binary_preds = (predictions_proba >= thresh).astype(int)\n",
        "    default_rate = sum(binary_preds)/len(binary_preds)\n",
        "    print(f\"   - Default rate at threshold {thresh}: {default_rate*100:.2f}%\")\n",
        "\n",
        "# Rekomendasi untuk tim bisnis\n",
        "print(\"\\nðŸ’¡ Business Recommendations:\")\n",
        "print(\"1. Prioritasi applicant dengan probability > 0.7 untuk review manual\")\n",
        "print(\"2. Applicant dengan probability < 0.3 dapat diapprove secara otomatis\")\n",
        "print(\"3. Buat segmentasi risk-based pricing berdasarkan probability score\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM2Xk1zzZ-Fw",
        "outputId": "668a0d4e-38b0-41f4-b496-68c0935a507e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“ˆ Prediction Analysis:\n",
            "   - Min probability: 0.0000\n",
            "   - Max probability: 1.0000\n",
            "   - Mean probability: 0.3283\n",
            "   - Default rate at threshold 0.3: 56.02%\n",
            "   - Default rate at threshold 0.5: 6.26%\n",
            "   - Default rate at threshold 0.7: 0.48%\n",
            "\n",
            "ðŸ’¡ Business Recommendations:\n",
            "1. Prioritasi applicant dengan probability > 0.7 untuk review manual\n",
            "2. Applicant dengan probability < 0.3 dapat diapprove secara otomatis\n",
            "3. Buat segmentasi risk-based pricing berdasarkan probability score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality check\n",
        "print(\"\\nðŸ” Quality Check:\")\n",
        "print(f\"   - Expected features: {len(feature_names)}\")\n",
        "print(f\"   - Features in new data: {new_data.shape[1]}\")\n",
        "print(f\"   - Missing values after imputation: {pd.DataFrame(new_data).isnull().sum().sum()}\")\n",
        "print(f\"   - Data shape consistency: {new_data_scaled.shape[1] == model.n_features_in_}\")\n",
        "\n",
        "# Sample predictions\n",
        "print(\"\\nðŸ‘€ Sample predictions:\")\n",
        "sample_results = results_df.head(10).copy()\n",
        "sample_results['TARGET_PROBA'] = sample_results['TARGET_PROBA'].round(4)\n",
        "print(sample_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO7vK70waCeY",
        "outputId": "9afa9494-6468-4df6-8f99-89df6dacae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ” Quality Check:\n",
            "   - Expected features: 130\n",
            "   - Features in new data: 130\n",
            "   - Missing values after imputation: 0\n",
            "   - Data shape consistency: True\n",
            "\n",
            "ðŸ‘€ Sample predictions:\n",
            "   SK_ID_CURR  TARGET_PROBA  TARGET_PREDICTION\n",
            "0      100001        0.1825                  0\n",
            "1      100005        0.3639                  0\n",
            "2      100013        0.4263                  0\n",
            "3      100028        0.4424                  0\n",
            "4      100038        0.4151                  0\n",
            "5      100042        0.2294                  0\n",
            "6      100057        0.2895                  0\n",
            "7      100065        0.2789                  0\n",
            "8      100066        0.3013                  0\n",
            "9      100067        0.3156                  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Executive summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ EXECUTIVE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model Used: {model_package['method_used']}\")\n",
        "print(f\"Model AUC: {model_package['performance_metrics']['auc']:.4f}\")\n",
        "print(f\"Optimal Threshold: {threshold:.3f}\")\n",
        "print(f\"Total Applicants: {len(results_df):,}\")\n",
        "print(f\"Recommended for Rejection: {sum(predictions_binary):,}\")\n",
        "print(f\"Predicted Default Rate: {sum(predictions_binary)/len(predictions_binary)*100:.1f}%\")\n",
        "print(f\"Estimated Cost Savings: ${model_package['performance_metrics']['business_impact']['Cost Savings']:,}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e2e32aQaGKS",
        "outputId": "4baaaf7a-0453-4a8c-c4f3-f905f5478668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“‹ EXECUTIVE SUMMARY\n",
            "============================================================\n",
            "Model Used: Hyperparameter Tuned\n",
            "Model AUC: 0.7374\n",
            "Optimal Threshold: 0.699\n",
            "Total Applicants: 48,744\n",
            "Recommended for Rejection: 241\n",
            "Predicted Default Rate: 0.5%\n",
            "Estimated Cost Savings: $85,000\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}